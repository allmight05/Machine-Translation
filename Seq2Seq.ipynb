{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description:\n",
    "\n",
    "In this project, I built a machine translation system from scratch to translate German sentences into English Using PyTorch and Python from scratch. I created my own encoder-decoder architecture—a type of sequence-to-sequence model. Here’s a simple breakdown:\n",
    "\n",
    "* Encoder: Reads the German sentence and converts it into a numerical format that captures its meaning.\n",
    "* Decoder: Uses these numbers and converts it into an English sentence.\n",
    "\n",
    "I trained and tested my model using the Multi30k dataset, which provides many pairs of German and English sentences. This project highlights my ability to design and build a deep learning model from the ground up without relying too much on pre-built libraries.\n",
    "\n",
    "This entire Project is built without TorchText library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\USERS\\CHALL\\ONEDRIVE\\DESKTOP\\VS CODE DESKTOP\\TORCH_ENV\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import Counter\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import spacy\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Checking the availability of GPU\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Device Should be set to GPU for faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To ensure deterministic results\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Loading the Data and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading German and English Tokenizers Using spaCy Library\n",
    "\n",
    "spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Tokenizer Functions\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset and Mapping it to the Tokenizer Functions\n",
    "\n",
    "dataset = load_dataset(\"bentrevett/multi30k\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return {\n",
    "        \"SRC\" : [tokenize_de(example) for example in examples[\"de\"]],\n",
    "        \"TRG\" : [tokenize_en(example) for example in examples[\"en\"]],\n",
    "    }\n",
    "    \n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Vocabulary with Padding(PAD), Unknown(UNK), Start of Sentence(SOS) and End of Sentence(EOS) Tokens\n",
    "\n",
    "def build_vocab(tokenized_data, min_freq=2):\n",
    "\n",
    "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1, \"<SOS>\": 2, \"<EOS>\": 3}\n",
    "    idx = 4\n",
    "\n",
    "    token_counter = Counter(token for tokens in tokenized_data for token in tokens)\n",
    "\n",
    "\n",
    "    for token, freq in token_counter.items():\n",
    "        if freq >= min_freq and token not in vocab:\n",
    "            vocab[token] = idx\n",
    "            idx += 1\n",
    "\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Vocabulary for Source and Target Languages\n",
    "\n",
    "src_vocab =  build_vocab(tokenized_dataset[\"train\"][\"SRC\"])\n",
    "trg_vocab = build_vocab(tokenized_dataset[\"train\"][\"TRG\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function to add SOS, UNK and EOS tokens.\n",
    "\n",
    "def numericalize(tokens, vocab):\n",
    "    return [vocab[\"<SOS>\"]] + [vocab.get(token, vocab[\"<UNK>\"]) for token in tokens] + [vocab[\"<EOS>\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Function to Numericalize the Tokens\n",
    "\n",
    "def numericalize_function(examples):\n",
    "    return {\n",
    "        \"SRC\": [numericalize(tokens, src_vocab) for tokens in examples[\"SRC\"]],\n",
    "        \"TRG\": [numericalize(tokens, trg_vocab) for tokens in examples[\"TRG\"]]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping the Numericalize_function to the Tokenized Dataset\n",
    "\n",
    "numericalized_dataset = tokenized_dataset.map(numericalize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the collate function to pad the sequences to the same length\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src = [torch.tensor(item[\"SRC\"]) for item in batch]\n",
    "    trg = [torch.tensor(item[\"TRG\"]) for item in batch]\n",
    "    src_padded = pad_sequence(src, batch_first=True, padding_value=src_vocab[\"<PAD>\"])\n",
    "    trg_padded = pad_sequence(trg, batch_first=True, padding_value=trg_vocab[\"<PAD>\"])\n",
    "    return src_padded, trg_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loaders for Train, Validation and Test Sets\n",
    "\n",
    "train_loader = DataLoader(numericalized_dataset[\"train\"], batch_size=128, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(numericalized_dataset[\"validation\"], batch_size=128, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(numericalized_dataset[\"test\"], batch_size=128, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC Batch Shape: torch.Size([128, 25])\n",
      "TRG Batch Shape: torch.Size([128, 26])\n",
      "Example SRC Batch: tensor([   2,    4,  580,  662,    8,    9,  193,  407,   49,   50,  566,  143,\n",
      "         204,  841, 3901,   28,    3,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0])\n",
      "Example TRG Batch: tensor([   2,   25, 3567, 3899,   52,  568,   49, 1786,   14,    3,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n"
     ]
    }
   ],
   "source": [
    "# Checking the data loader\n",
    "\n",
    "for (src_batch, trg_batch) in train_loader:\n",
    "    print(\"SRC Batch Shape:\", src_batch.shape)\n",
    "    print(\"TRG Batch Shape:\", trg_batch.shape)\n",
    "    print(\"Example SRC Batch:\", src_batch[0])\n",
    "    print(\"Example TRG Batch:\", trg_batch[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder Architecture\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout, device):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.device = device \n",
    "    \n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        #embedded = [batch size, src len, emb dim]\n",
    "        batch_size = src.shape[0] \n",
    "\n",
    "        # Initialize hidden and cell states explicitly\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hid_dim).to(self.device) \n",
    "        cell = torch.zeros(self.n_layers, batch_size, self.hid_dim).to(self.device)\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        # outputs = [batch size, src len, hid dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hid dim]\n",
    "        # cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        return hidden, cell\n",
    "#Here outputs is the hidden states for all time steps in top layer of LSTM.[src_len, batch_size, hid_dim].\n",
    "#(hidden, cell): Final hidden and cell states from all LSTM layers. Shape of each: [n_layers, batch_size, hid_dim].\n",
    "#src_len is  sequence length.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decoder Architecture\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        # input = [batch size]\n",
    "\n",
    "        # n directions in the decoder will both always be 1, therefore:\n",
    "        # hidden = [n layers, batch size, hid dim]\n",
    "        # context = [n layers, batch size, hid dim]\n",
    "        \n",
    "        input = input.unsqueeze(1)\n",
    "        # input = [batchsize, 1]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "\n",
    "        # seq len and n directions will always be 1 in the decoder, therefore:\n",
    "        # output = [batch size, 1, hid dim]\n",
    "        # hidden = [n layers, batch size, hid dim]\n",
    "        # cell = [n layers, batch size, hid dim]\n",
    "        \n",
    "        prediction = self.fc_out(output.squeeze(1))\n",
    "        # prediction = [batchsize, output_dim]\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq2Seq Model which combines Encoder and Decoder.\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "        assert (\n",
    "            encoder.hid_dim == decoder.hid_dim\n",
    "        ), \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert (\n",
    "            encoder.n_layers == decoder.n_layers\n",
    "        ), \"Encoder and decoder must have equal number of layers!\"\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "\n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "\n",
    "        hidden, cell = self.encoder(src)\n",
    "\n",
    "        input = trg[:, 0]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "\n",
    "            outputs[:, t, :] = output\n",
    "\n",
    "            \n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.Initializing the Hyperparameters and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters for further use\n",
    "\n",
    "INPUT_DIM = len(src_vocab)\n",
    "OUTPUT_DIM = len(trg_vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT, device)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(8014, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(6191, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=512, out_features=6191, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing the weights of the model\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 14,168,879 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# Checking for number of trainable parameters in the model.\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the optimizer\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the loss function\n",
    "\n",
    "pad_token = \"<PAD>\"\n",
    "PAD_IDX = trg_vocab[pad_token]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the training loop\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion, clip):\n",
    "    model.train()  \n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch_idx, (src, trg) in enumerate(dataloader):\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device) \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        output = output[:, 1:].reshape(-1, output_dim)\n",
    "        trg = trg[:, 1:].reshape(-1)\n",
    "\n",
    "        loss = criterion(output, trg)  \n",
    "\n",
    "        loss.backward()  \n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step() \n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the evaluation loop\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()  \n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (src, trg) in enumerate(dataloader):\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            output = model(src, trg, 0)\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            output = output[:, 1:].reshape(-1, output_dim)\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, trg)  \n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    return epoch_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function to calculate the elapsed time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Saving the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 31s\n",
      "\tTrain Loss: 5.069 | Train PPL: 158.957\n",
      "\t Val. Loss: 5.083 |  Val. PPL: 161.185\n",
      "Epoch: 02 | Time: 0m 32s\n",
      "\tTrain Loss: 4.483 | Train PPL:  88.495\n",
      "\t Val. Loss: 4.727 |  Val. PPL: 112.985\n",
      "Epoch: 03 | Time: 0m 32s\n",
      "\tTrain Loss: 4.162 | Train PPL:  64.220\n",
      "\t Val. Loss: 4.546 |  Val. PPL:  94.232\n",
      "Epoch: 04 | Time: 0m 32s\n",
      "\tTrain Loss: 3.953 | Train PPL:  52.073\n",
      "\t Val. Loss: 4.433 |  Val. PPL:  84.169\n",
      "Epoch: 05 | Time: 0m 33s\n",
      "\tTrain Loss: 3.790 | Train PPL:  44.272\n",
      "\t Val. Loss: 4.419 |  Val. PPL:  83.045\n",
      "Epoch: 06 | Time: 0m 32s\n",
      "\tTrain Loss: 3.682 | Train PPL:  39.715\n",
      "\t Val. Loss: 4.296 |  Val. PPL:  73.390\n",
      "Epoch: 07 | Time: 0m 31s\n",
      "\tTrain Loss: 3.552 | Train PPL:  34.871\n",
      "\t Val. Loss: 4.239 |  Val. PPL:  69.314\n",
      "Epoch: 08 | Time: 0m 34s\n",
      "\tTrain Loss: 3.421 | Train PPL:  30.587\n",
      "\t Val. Loss: 4.127 |  Val. PPL:  62.022\n",
      "Epoch: 09 | Time: 0m 32s\n",
      "\tTrain Loss: 3.323 | Train PPL:  27.738\n",
      "\t Val. Loss: 4.094 |  Val. PPL:  59.961\n",
      "Epoch: 10 | Time: 0m 33s\n",
      "\tTrain Loss: 3.213 | Train PPL:  24.842\n",
      "\t Val. Loss: 4.046 |  Val. PPL:  57.151\n",
      "Epoch: 11 | Time: 0m 32s\n",
      "\tTrain Loss: 3.127 | Train PPL:  22.798\n",
      "\t Val. Loss: 3.972 |  Val. PPL:  53.067\n",
      "Epoch: 12 | Time: 0m 31s\n",
      "\tTrain Loss: 3.017 | Train PPL:  20.438\n",
      "\t Val. Loss: 3.919 |  Val. PPL:  50.354\n",
      "Epoch: 13 | Time: 0m 32s\n",
      "\tTrain Loss: 2.942 | Train PPL:  18.962\n",
      "\t Val. Loss: 3.843 |  Val. PPL:  46.662\n",
      "Epoch: 14 | Time: 0m 32s\n",
      "\tTrain Loss: 2.832 | Train PPL:  16.980\n",
      "\t Val. Loss: 3.845 |  Val. PPL:  46.750\n",
      "Epoch: 15 | Time: 0m 32s\n",
      "\tTrain Loss: 2.763 | Train PPL:  15.844\n",
      "\t Val. Loss: 3.788 |  Val. PPL:  44.179\n"
     ]
    }
   ],
   "source": [
    "# Training the model and saving the best model\n",
    "\n",
    "N_EPOCHS = 15\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, val_loader, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"Seq model.pt\")\n",
    "\n",
    "    print(f\"Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}\")\n",
    "    print(f\"\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we trained the model for 15 epochs and achieved a validation loss of 3.7 and validation perplexity of 44.1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chall\\AppData\\Local\\Temp\\ipykernel_11796\\399348693.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"Seq model.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 3.769 | Test PPL:  43.348 |\n"
     ]
    }
   ],
   "source": [
    "# Loading the model for testing\n",
    "\n",
    "model.load_state_dict(torch.load(\"Seq model.pt\"))\n",
    "\n",
    "test_loss = evaluate(model, test_loader, criterion)\n",
    "\n",
    "print(f\"| Test Loss: {test_loss:.3f} | Test PPL: {np.exp(test_loss):7.3f} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_trg_vocab = {i: w for w, i in trg_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to translate the German sentence to English\n",
    "\n",
    "def translate_sentence(sentence, src_vocab, trg_vocab, model, device, max_len=50):\n",
    "    \n",
    "    # Tokenize the German sentence and reverse tokens\n",
    "    tokens = [tok.text for tok in spacy_de.tokenizer(sentence)][::-1]\n",
    "    numericalized = (\n",
    "        [src_vocab[\"<SOS>\"]]\n",
    "        + [src_vocab.get(token, src_vocab[\"<UNK>\"]) for token in tokens]\n",
    "        + [src_vocab[\"<EOS>\"]]\n",
    "    )\n",
    "    tensor = torch.LongTensor(numericalized).unsqueeze(0).to(device)  # shape: [1, seq_len]\n",
    "\n",
    "    # Encode the source sentence\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(tensor)\n",
    "\n",
    "    # Initialize the decoder input with <SOS>\n",
    "    trg_indexes = [trg_vocab[\"<SOS>\"]]\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n",
    "        pred_token = output.argmax(1).item()\n",
    "        trg_indexes.append(pred_token)\n",
    "        if pred_token == trg_vocab[\"<EOS>\"]:\n",
    "            break\n",
    "\n",
    "    # Convert predicted indices to tokens and remove <SOS> and <EOS>\n",
    "    translated_tokens = [inv_trg_vocab[idx] for idx in trg_indexes]\n",
    "    return translated_tokens[1:-1]\n",
    "\n",
    "# Example test sentences (German) and their reference English translations\n",
    "test_sentences = [\n",
    "    \"Ein kleines Mädchen spielt mit einem Ball im Park.\",\n",
    "    \"Ein Mann fährt ein rotes Auto.\",\n",
    "    \"Zwei Katzen sitzen auf einem Baum.\"\n",
    "]\n",
    "\n",
    "reference_translations = [\n",
    "    [\"A\", \"little\", \"girl\", \"plays\", \"with\", \"a\", \"ball\", \"in\", \"the\", \"park\", \".\"],\n",
    "    [\"A\", \"man\", \"is\", \"driving\", \"a\", \"red\", \"car\", \".\"],\n",
    "    [\"Two\", \"cats\", \"are\", \"sitting\", \"in\", \"a\", \"tree\", \".\"]\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:  Ein kleines Mädchen spielt mit einem Ball im Park.\n",
      "Reference Translation:  A little girl plays with a ball in the park .\n",
      "Predicted Translation:  A little girl plays playing a a on a beach .\n",
      "\n",
      "Source:  Ein Mann fährt ein rotes Auto.\n",
      "Reference Translation:  A man is driving a red car .\n",
      "Predicted Translation:  A man is riding a a . .\n",
      "\n",
      "Source:  Zwei Katzen sitzen auf einem Baum.\n",
      "Reference Translation:  Two cats are sitting in a tree .\n",
      "Predicted Translation:  Two men sitting on a bench .\n",
      "\n",
      "Corpus BLEU score: 0.17550802145917324\n"
     ]
    }
   ],
   "source": [
    "# Translating the test sentences and calculating the BLEU score\n",
    "\n",
    "predicted_translations = []\n",
    "for sentence, ref in zip(test_sentences, reference_translations):\n",
    "    translation = translate_sentence(sentence, src_vocab, trg_vocab, model, device)\n",
    "    print(\"Source: \", sentence)\n",
    "    print(\"Reference Translation: \", \" \".join(ref))\n",
    "    print(\"Predicted Translation: \", \" \".join(translation))\n",
    "    print()\n",
    "    predicted_translations.append(translation)\n",
    "\n",
    "bleu_score = corpus_bleu([[ref] for ref in reference_translations], predicted_translations)\n",
    "print(\"Corpus BLEU score:\", bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* our German-to-English translation model built with an encoder-decoder architecture achieved a validation loss of 3.7, a perplexity of 44.1, and a BLEU score of 0.17. While these metrics might seem bad at first glance, they are good given the limited training time and resources available for this project. \n",
    "* The results demonstrate that even a relatively small and beginner model can capture meaningful patterns in language translation.\n",
    "* With additional data, further hyperparameter tuning, and potentially more good architectures, there is room for improvement. \n",
    "\n",
    "Overall, this project provides a solid foundation and valuable insights for future work in neural machine translation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
